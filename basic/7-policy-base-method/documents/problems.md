はい、強化学習で方策ベースの手法を用いる練習問題を3問出題します。  
理解度や実装力を高めるための内容になっています。

---

### 【問題1】  
#### カートポール環境での方策勾配法（REINFORCE）の実装

OpenAI Gymの`CartPole-v1`環境を使い、  
**方策勾配法（REINFORCE）**でエージェントを学習させてください。

- 方策ネットワークは多層パーセプトロン（MLP）で構いません。
- エピソードごとに累積報酬を計算し、パラメータを更新してください。
- 学習曲線（エピソードごとの報酬推移）もプロットしてください。

---

### 【問題2】  
#### ガウス方策による連続制御

OpenAI Gymの`MountainCarContinuous-v0`環境を使い、  
**連続行動空間向けの確率的方策（ガウス分布）**を用いた方策勾配法を実装してください。

- 方策ネットワークは、状態を入力し、行動の平均と分散を出力するようにしてください。
- 行動はそのガウス分布からサンプリングしてください。
- 学習が進むにつれて報酬がどのように推移するか観察してください。

---

### 【問題3】  
#### Actor-Critic法による学習

OpenAI Gymの`Acrobot-v1`環境で、  
**Actor-Critic法**を用いてエージェントを学習させてください。

- Actor（方策）とCritic（価値関数）は、それぞれ別のネットワークで実装してください。
- TD誤差を用いてCriticを更新し、Advantageを使ってActorを更新してください。
- 学習曲線をプロットし、学習の進み具合を可視化してください。

---

#### ※いずれもPython（PyTorchやTensorFlowなど）での実装が推奨です。

---

**チャレンジしてみてください！**

以上です。