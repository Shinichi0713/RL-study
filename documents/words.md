## オンライン学習
エージェントが環境と相互作用しながらリアルタイムで学習していく方法。  
この学習法は、エージェントが行動を選択し、その結果として得られる報酬をもとに、次の行動を改善していくプロセスを繰り返します。  

### オンライン学習の特徴
#### リアルタイムの更新
エージェントは環境と相互作用しながら、逐次的に学習を行います。各ステップで得られる情報を即座に利用して、ポリシー（行動選択の戦略）や価値関数（状態の価値）を更新します。

#### 逐次的なデータ処理
データが逐次的に提供されるため、エージェントは過去の経験を蓄積しながら学習します。これにより、エージェントは環境の変化に迅速に適応できます。

#### 探索と利用のバランス
エージェントは未知の環境を探索しながら、既知の情報を利用して最適な行動を選択します。このバランスを保つことが、オンライン学習の成功に不可欠です。

#### リアルタイムのフィードバック
エージェントは行動の結果として得られる報酬をリアルタイムで受け取り、そのフィードバックをもとに学習を進めます。これにより、エージェントは迅速に適応し、効率的な行動を選択できるようになります。

### オンライン学習が出来る条件
1. リアルタイムデータの利用
エージェントが環境とリアルタイムで相互作用できることが重要です。これにより、逐次的にデータを収集し、即座に学習を進めることができます。

2. 即時のフィードバック
エージェントが行動を選択した後、即時に報酬や次の状態に関するフィードバックを受け取ることができる必要があります。これにより、迅速にポリシーや価値関数を更新できます。

3. 適応性の高いアルゴリズム
環境の変化に迅速に適応できるアルゴリズムを使用することが重要です。例えば、Q学習やSARSAなどの強化学習アルゴリズムが一般的に使用されます。


## 概要
メカニズムはモンテカルロ法とTD法とを統合したもの。

適格度トレース（eligibility trace）は強化学習の基本的なメカニズムの1つ。
TD法を適格度トレースによって補強すると、モンテカルロ法を一方の端、1ステップTD法を他方の端とした範囲にまたがる一連の手法が作り出されます。  

適格度トレースは理論的には前方観測的な見方（forward view）と呼ばれ、技術的には後方観測的な見方（backward view）と呼ばれます。

- 前方観測的な見方: 適格度トレースを用いた手法で何が計算されるのかを理解するのに有用
- 後方観測的な見方: アルゴリズムそれ自体の直感を発展させるのに適している

もう一つの解釈。  
強化学習（Reinforcement Learning）のアルゴリズムにおいて、過去の状態や行動に対する報酬の影響を考慮するための手法  
特に時間差分学習（Temporal Difference Learning）と組み合わせて使用されます。適格度トレースは、エージェントが過去の行動と現在の報酬を関連付けて学習する能力を向上させます。  

## 基本概念
適格度トレースは、各状態や状態-行動ペアがどれだけ「適格」であるか、すなわち報酬に対する責任がどれだけあるかを示す値を保持します。  
これにより、エージェントは単一のステップだけでなく、複数のステップにわたって報酬の影響を学習することができます。  
![image](https://github.com/user-attachments/assets/4ad969b1-3cf3-4a23-9d8c-2426c6fe0dc1)


## TD(λ)法
適格度トレースの考えのルーツとなる学習法がTD(λ)。  
![image](https://github.com/user-attachments/assets/82e1d0ea-d402-413b-b4c2-a1915d004b98)

各段がそれぞれ
$n$
ステップTD法になっていて、それぞれの
$n$
ステップ収益と現在の推定価値との差分について、それぞれの重みで学習を行っている。

#### 課題
問題は、これだと
$n$
ステップ先にならないと現在の推定価値を更新できないということ。
それだと、更新が生じるまでにだいぶ待つ必要が出てくる。

**解決法**
__今__：  
時間ステップ
$t$
から見て、時間ステップ
$t+n$
での推定価値との差分を、時間ステップ
$t$
での価値の推定に使おうとしている

__改善後__：  
時間ステップ
$t+n$
から見て、その推定価値との差分が、時間ステップ
$t$
の時点の価値の推定にどれくらい影響を与えているのかを考えてみる。

```math
e_{s,a}^{(t)} = 
\begin{cases} 
0 & (t = 0) \\
\gamma \lambda e_{s,a}^{(t-1)} + 1 & (s = s_t, a = a_t) \\
\gamma \lambda e_{s,a}^{(t-1)} & (\text{otherwise})
\end{cases}
```


## 総括
適格度トレースは、強化学習において過去の経験を効率的に利用するための強力な手法  
これにより、エージェントは単一のステップだけでなく、複数のステップにわたる報酬の影響を学習することができます。  
適格度トレースを使用することで、より高速で効率的な学習が可能になります。  
